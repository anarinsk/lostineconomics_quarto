---

layout: post
toc: false
comments: true
title:  Standard Error 
description: 표준 오차에 관한 개인적인 정리 
categories: [statistics, personal-use]

---

## 왜? 

통계학을 들은 사람들이라면 표준 오차는 대체로 잘 아는 내용이다. 순전히 개인적인 정리를 위한 용도다. 나이를 드니까 명민함이 떨어지고 당연하게 알고 있었던 것도 모르게 되는 경우가 많으니까. 

## Sample Mean 

독립적인 무작위 추출로 크기 $n$의 표본 $x_1, \dotsc, x_n$을 얻었다고 하자. $x_i$가 모평균 $\mu$와 모 표준편차 $\sigma$를 따른다고 할 때, 표본 평균은 다음과 같다. 

$$
\overline{X}_n = \sum_{i=1}^n x_i.
$$
- $\mathbb E(\overline X_n) = \mu$ 
- ${\rm Var} (\overline X_n) = {\sigma^2}/{n}$

덤으로 Central Limit Theorem에 따르면 다음에 성립한다. 

- $\overline X_n \sim {\rm N} (\mu, {\sigma^2}/{n})$ 

### Standard Error 

어떤 통계량의 표준 오차는 (표본) 통계량의 표준 편차를 의미한다. 위 식에서 ${\rm Var} (\overline x)$을 의미한다. 많이 사용되는 표본 평균의 표준 오차는 SEM(Standard Error of the Mean)이라고도 한다. 표본 평균 $\overline X_n$의 표준 편차를 생각해보자. $\sigma^2 < \infty$를 가정할 때, 

$$
\mathbb E(\overline X_n - \mu)^2 = \mathbb E (\dfrac{\sum(x_i -\mu)}{n})^2 = \dfrac{1}{n^2} n \sigma^2 = \dfrac{\sigma^2}{n}
$$

그런데 우리는 $\sigma$를 알 도리가 없다. 즉, SEM은 $\sigma/\sqrt{n}$이지만 $\sigma$를 알도리가 없으므로 이 값 역시 추정의 대상이다. 얼른 생각할 수 있는 추정량 $S_n$는 다음과 같다.  

$$
S^2_n = \dfrac{1}{n-1} \sum_{i=1}^{n} (x_i - \overline{x})^2
$$

몇가지 가정이 필요하지만, 대수의 법칙에 따라서 $S_n^2 \overset{P}{\longrightarrow} \sigma^2$가 성립하고, $S^2$은 $\sigma^2$의 일치 추정량이다. 이 문제는 조금 뒤에 다시 살펴보자. $\overset{P}{\longrightarrow}$는 확률적인 수렴, 즉 probability limit을 뜻한다. 

### Unbiased and Consistent 

- 표본 평균은 모평균의 불편 추정량이다. 즉, $\mathbb E(\overline X_n) = \mu$
- 표본 평균은 모평균의 일치 추정량이다. 즉, $\sigma^2$의 값은 모르지만 $n$이 커짐에 따라서 표본 평균의 분산이 0이 된다. 우선 대수의 법칙에 따라서 다음의 결과가 성립한다.  $\lim_{n \to \infty} \overline X_n = \mu$. 
- 대수의 법칙 없이 설명한다면? $\mathbb E (\overline X_n) = \mu$가 성립한다. 즉, $\overline X_n$의 분포를 고려할 때 표본 평균의 기댓값은 $\mu$이다. 이때, 표본 평균의 분산은 앞서 계산한 표준오차의 제곱이다. 즉, $\rm Var(\overline X_n) = \dfrac{\sigma^2}{n}$. 모분산 $\sigma^2 < \infty$를 가정했기 때문에 $n$이 증가함에 따라서 $\rm Var$이 0으로 수렴한다. 따라서 $\overline{X}$는 일치 추정량이다. 

## More on $S^2$

### $(n-1)$ in denominator 

$S^2$의 분모에 왜 $(n-1)$이 들어갈까? 불편 추정량을 얻기 위해서다. 불편 추정량이란 해당 통계치의 기대값이 모수가 되는 추정량을 의미한다. 직접 $S^2$의 기대값을 구해보자. 먼저, 

$$
\begin{aligned}
\sigma^2 = {\rm Var}(x_i) & = \mathbb E[(x_i - \mu)^2] \\
& = \mathbb E [x_i^2 - 2 x_i \mu + \mu^2] \\
& = \mathbb E(x_i^2) - 2\mu E(x_i) + \mu^2 \\
& =  \mathbb E(x_i^2) - \mu^2
\end{aligned}
$$

$\mathbb E(x_i^2) = \sigma^2 + \mu^2$임을 기억해두자. 

이제 $S^2$의 기대값을 구해보자. 분자 먼저 계산하자. 

$$
\begin{aligned}
\mathbb E[\sum(x_i - \overline{x})^2] & = \mathbb E[\sum (x_i^2 - 2 x_i \overline{x} + \overline{x}^2)] \\
& = \mathbb E[\sum x_i^2 - n 2 \overline{x} \cdot \overline{x} + n \overline{x}^2] \\
& = \mathbb E[\sum x_i^2 - n \overline{x}^2)] \\
& = \sum \mathbb E(x_i^2) - n \mathbb E(\overline{x}^2)
\end{aligned}
$$

합해지는 부분의 각각을 따져보자. 

$$
\begin{aligned}
\sum \mathbb E(x_i^2) = n (\sigma^2 + \mu^2)
\end{aligned}
$$

$$
\begin{aligned}
\mathbb E(\overline{x}^2) & = \mathbb E[(\dfrac{x_1 + x_2 + \dotsb + x_n}{n})^2] \\
\end{aligned}
$$

 - $\mathbb E[(\dfrac{x_1 + x_2 + \dotsb + x_n}{n})^2]$의 분자를 계산하면 된다. 먼저 각 $x_i$의 제곱이 $n$개 나온다. 다음으로 각각 $x_i$가 독립적으로 추출되었으므로 $i \neq j$일 때 $\mathbb E(x_i x_j) = \mathbb E(x_i) \mathbb E(x_j) = \mu^2$이 된다. $n$ 개 중에서 2개를 순서에 관계 없이 뽑게 된다. 즉, 

$$
\mathbb E[(x_1 + x_2 + \dotsb + x_n)^2]  = n (\sigma^2 + \mu^2) + 2 \dfrac{n(n-1)}{2!} \mu^2 = n \sigma^2 + n^2 \mu^2
$$

따라서, 

$$
\mathbb E(\overline{x}^2)  = \dfrac{\sigma^2}{n} + \mu^2 
$$

이제 각각을 넣어 계산을 완료하자. 

$$
\begin{aligned}
\mathbb E[\sum(x_i - \overline{x})^2] & = \sum \mathbb E(x_i^2) - n \mathbb E(\overline{x}^2) \\
& = n \sigma^2 + n \mu^2 - \sigma^2 - n \mu^2 \\
& = (n-1) \sigma^2 
\end{aligned}
$$

따라서 불편 추정량이 되기 위해서는 $S^2$의 분모에 $n-1$이 필요하다. 

### Consistency

이제 일치성을 알아보자. 엄밀한 증명은 생략하고 간략하게 아이디어만 적도록 하겠다.[^1] $\rm Var (S^2_n) < \infty$를 가정하자. 이때 

$$
\begin{aligned}
S_n^2 & = \dfrac{1}{n-1}\sum_{i=1}^n (X_i - \overline{X}_n) \\
& = \dfrac{n}{n-1}(\dfrac{\sum X_i^2}{n} - \overline X_n) \\
& \stackrel{P}{\longrightarrow} 1\cdot(\mathbb E(X^2)-\mu^2) = \sigma^2
\end{aligned}
$$

[^1]: 자세한 내용은 [여기](http://www.utstat.toronto.edu/~brunner/oldclass/413f08/handouts/STA413Ch4.pdf) 책의 해당 부분을 참고하라. 

## t-distribution 

Gosset이 제안한 $t$분포는 정규 분포에서 온 분포다. 원래 형태는 다음과 같다. 

$$
\dfrac{Z}{V/v}
$$

$Z$는 표준정규분포를, $V$는 자유도가 $v$인 카이제곱 분포를 나타낸다. 

### $S^2$이 따르는 분포  

$S^2$이 $(n-1)$의 자유도를 따르는 카이제곱 분포를 따를까? 간단히 계산해보자. 우선 몇 가지 결과를 먼저 얻어보자. 
iid로 표집된 $Z_1, \dotsc, Z_n$이 있다고 하자. 

- $Var(\overline{Z}) = {1}/{n}$
- $\sqrt{n} \overline{Z} \sim N(0,1)$
- $n \overline{Z}^2 \sim \chi^2_1$

이 결과를 이용해 다음을 증명해보자.  

$$
\sum_{i=1}^n (Z_i - \overline{Z}) + n \overline{Z}^2 = 
\sum_{i=1}^n Z_i^2 - 2(n \overline{Z}) \overline{Z} + n \overline{Z}^2 + n \overline{Z}^2 = \sum_{i=1}^n Z_i^2
$$

즉, $\sum_{i=1}^n (Z_i - \overline{Z}) \sim \chi^2_{n-1}$이다. 

이제 다음을 확인해보자. 

$$
\begin{aligned}
(n-1) \dfrac{s^2}{\sigma^2} & = \dfrac{1}{\sigma^2} \sum_{i=1}^n (x_i - \overline{x})^2 \\
& = \dfrac{1}{\sigma^2} \sum (\mu + \sigma Z_i - (\mu + \sigma \overline{Z}) )^2 \\
& = \dfrac{1}{\sigma^2} \sigma^2 \sum  (Z_i -\overline{Z}) ^2 \\
& = \sum (Z_i - \overline{Z})^2 \sim \chi^2_{n-1}
\end{aligned}
$$

보통 평균의 신뢰구간을 추정하는 과정에서는 표본 분산을 추정하는 데 평균 값 하나를 쓰게 되고, 따라서 자유도가 1 줄어들게 된다. $t$ 분포는 $n$이 증가함에 따라서 정규 분포에 근사하기 때문에 대부분의 교과서에서는 $n \geq 30$ 이상에서는 대충 정규 분포를 써도 된다고 이야기하고 있다. 

## Regression Coefficients 

회귀 계수를 검정할 때는 보통 $t$분포를 쓴다. 왜 그럴까? 매트릭스 형태로 회귀 모형을 다시 음미해보자. 

$$
\hat \beta = (X^T X)^{-1}(X y)
$$

- $y \sim \mathrm N(X\beta, \sigma^2 I_n)$

따라서, $\hat \beta$의 분포는 다음과 같다. 

$$
\hat \beta \sim \mathrm N (\beta, \sigma^2 (X^T X)^{-1})
$$

따라서 $\hat \beta$는 정규 분포를 지니게 된다. 따라서 $\hat \beta_i$를 검정할 때 t-분포가 등장할 수 있다. 즉, 

$$
t_i = \dfrac{\hat \beta_i - \beta}{\sigma \sqrt{(X^T X)_{ii}^{-1}}} \sim \mathrm N (0, 1)
$$

우리는 $\sigma$를 알 수 없으므로 $\sigma$를 $S^2$으로 추정해야 한다. 회귀 모형이 $p$개의 파라미터를 추정한다고 하자. 즉, $\beta_1, \dotsc, \beta_k$를 추정한다. 이때 $S^2$은 다음과 같다. 

$$
S^2 = \dfrac{1}{n-p} \sum_{i=1}^n (y_i -  x^T_i \hat \beta)
$$

- $x^T_i$: $X^T$의 $i$ 번째 컬럼 벡터 

따라서 통계량 $t_i$의 분자는 정규 분포를 분모는 자유도 $(n-p)$의 카이제곱 분포를 따른다. 