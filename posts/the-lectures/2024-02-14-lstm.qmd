---
title: "Long Short-Term Memory (LSTM) Network" 
description: RNN의 문제를 해결해보자.  
author: "JS HUHH"
date: "02/14/2024"
image: "./images/cat-network.webp"
categories: [the-lectures, machine-learning]
#fig-align: center
filters:
   - lightbox
   #- line-highlight
lightbox: auto
#jupyter: python3
draft: false
format:
  html:
    code-fold: false
    code-block-bg: true
    highlight-style: github
---

## TL; DR
LSTM을 찍먹해보자. 

{{< video https://www.youtube.com/watch?v=YCzL96nL7j0 >}}

## 넋두리 

RNN의 가장 큰 문제는 웨이트의 크기에 따라서 효과가 지나치게 증폭되거나 지나치게 줄어드는 데 있다. 경사하강법을 사용할 때 최적 파라미터의 범위가 지나치게 크게 변하거나 지나치게 작게 변하면 원활한 최적화가 어렵다. 이러한 문제를 막기 위해 웨이트의 크기가 제한된다면 이는 그 자체로 최적화에 위배된다. 이러한 문제를 완화하기 위해 제안된 네트워크 구조가 LSTM이다. 

이걸 조금 말로 풀어보자. Long Short-term Memory란 말 그대로 과거의 정보가 미래로 전달되는 경로를 장기와 단기로 나누겠다는 것이다. 아래 기본 구조에서 보듯이 시점 혹은 노드의 중요도에 따라서 해당 노드가 뒤에 올 노드에 미치는 영향의 정도가 다를 수 있다. 그런데 RNN은 하나의 단일한 엣지 혹은 경로를 통해서 두 가지 정보, 즉 해당 노드가 바로 다음에 오는 노드에 미치는 영향과 더 멀리 떨어진 노드에 미치는 영향을 하나로 뭉쳐서 전달힌다. 이 과정에서 정보의 손실이 발생할 수 있다. LSTM 모델이란 이 경로를 두 개로 각각 분리한 것이다. 

:::{layout="[-1, 4, -1]"}
![RNN의 문제점; KABOOM!!!은 explode, poof!!! vanish를 의미한다.](./images/lstm/01.png){ style="border: 1px solid #000;" }
:::

## 기본 구조 

LSTM의 기본 구조는 복잡해보이지만, 아이디어는 단순하다. 

::: {layout="[[5, -1, 5, -1, 5]]" layout-valign="bottom"}
![LSTM의 아이디어; 장기와 단기의 기억을 분리해서 각각을 업데이트한다.](./images/lstm/02.png){ style="border: 1px solid #000;" }

![LSTM의 기본 구조](./images/lstm/03.png){ style="border: 1px solid #000;" }

![장기 기억에는 웨이트나 바이어스가 없다.](./images/lstm/06.png){ style="border: 1px solid #000;" }
:::

## Three Gates

원래 용어에 따르면 LSTM은 세 개의 게이트로 구성되어 있다. 

1. Forget Gate; 장기 기억을 할인하는 과정을 의미한다. 
2. Input Gate; 새로운 정보를 장기 기억에 추가하는 과정을 의미한다.
3. Output Gate; 장기 기억을 바탕으로 새로운 단기 정보를 생성하는 과정을 의미한다.

전기에서 넘어온 장기 기억과 단기 기억이 존재한다고 가정하자. 

업데이트에는 Sigmoid 함수와 Tanh 함수를 사용한다. 두 함수의 특성을 보면 활용하는 이유를 알 수 있다. Sigmoid 함수는 0과 1 사이의 값을 가지며, Tanh 함수는 -1과 1 사이의 값을 가진다. 전자는 기억을 할인하는 비율을 결정하고 후자는 기억 자체의 크기를 결정한다. 

::: {layout="[[5, -1, 5]]" layout-valign="bottom"}
![Sigmoid 함수](./images/lstm/04.png){ style="border: 1px solid #000;" }

![Tanh 함수](./images/lstm/05.png){ style="border: 1px solid #000;" }
:::

아래와 같이 표로 정리해보자. 

| 게이트 | 업데이트 정보 | Sigmoid | Tanh |
| :---: | --- | :---: | :---: |
| Forget Gate | 장기 기억 | 단기 기억, 인풋 | - |
| Input Gate  | 포겟 게이트의 장기 기억 | 단기 기억, 인풋 | 단기 기억, 인풋 |
| Output Gate | 단기 기억 | 단기 기억, 인풋 | 인풋 게이트의 장기 기억 |

### Forget Gate 

단기 기억과 인풋을 받아서 장기 기억을 할인한다.

### Input Gate

인풋이 장기기억을 어떻게 형성하는지를 표현한다. 이때 전기의 단기 기억과 인풋이 사용된다. 인풋과 전기의 단기 기억이 장기 기억의 크기를 Tanh 함수를 통해서 결정하고, Sigmonoid 함수를 통해서 얼마나 장기 기억에 추가할지를 결정한다. 

앞서 Forget Gate에서 할인된 장기 기억에 이 값을 더해 새 장기기억이 결정된다. 

### Output Gate

업데이트된 장기기억, 전기의 단기기억, 그리고 인풋 세 값이 단기기억을 업데이트한다. 장기 기억의 일부가 단기 기억으로 전환된다. 다만 앞서 인풋을 통해 장기기억을 업데이트했으므로 Output Gate에서 인풋과 전기의 단기기억은 단기 기억 자체의 크기에는 영향을 주지 않고 비율에만 영향을 준다. 

## How to Work 

이렇게 하나의 유닛이 매 기에 들어오는 정보를 처리한다고 보면 된다. 아래 예시에서 1기에 정보가 세 단계의 LSTM 유닛을 거쳐서 처리되 이런 유닛들이 이어져 마지막 정보까지 처리된다. 

::: {layout="[[5, -1, 5]]" layout-valign="bottom"}
![Day 1의 LSTM 처리](./images/lstm/10.png){ style="border: 1px solid #000;" }

![Day4까지의 처리를 통해 Day 5 예측](./images/lstm/11.png){ style="border: 1px solid #000;" }
:::


